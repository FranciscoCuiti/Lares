{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Funcion normalizar texto",
   "id": "4fe42113dcfe8a4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Modificar la función de preprocesamiento para manejar NaN\n",
    "def preprocesar_texto(texto):\n",
    "    if pd.isna(texto):\n",
    "        return ''\n",
    "    texto = str(texto).lower()\n",
    "    texto = re.sub(r'[^\\w\\s]', '', texto)\n",
    "    texto = ' '.join(texto.split())  # Eliminar espacios en blanco adicionales\n",
    "    return texto\n",
    "\n",
    "# Aplicar el preprocesamiento a la columna de descripciones en data_2021\n",
    "data_2021['descripcion_normalizada'] = data_2021['descripcion_sin_saltos'].apply(preprocesar_texto)\n",
    "\n",
    "# Verificar los primeros resultados\n",
    "print(data_2021[['descripcion_sin_saltos', 'descripcion_normalizada']].head())"
   ],
   "id": "e5743eeded2b3801"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Detección de duplicados por región y semestre con vectorización TD-IDF y la similaridad coseno",
   "id": "b7560f320ea24305"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "\n",
    "# Filtrar las filas que son de enero de 2021 en adelante\n",
    "# data_2021 = data_2021[data_2021['publicada'] >= '2021-01-01']\n",
    "\n",
    "# Asegurarse de que la columna 'duplicados_con' existe en el DataFrame\n",
    "data_2021['duplicados_con'] = [[] for _ in range(len(data_2021))]  # Crear la columna vacía\n",
    "\n",
    "# Definir el umbral de similaridad\n",
    "umbral = 0.85\n",
    "\n",
    "# Crear una lista para almacenar los resultados de duplicados de todas las regiones\n",
    "duplicados_totales = []\n",
    "\n",
    "# Recorrer cada región\n",
    "for region, grupo_region in data_2021.groupby('region'):\n",
    "    print(f\"Procesando duplicados para la región: {region}\")\n",
    "\n",
    "    # Recorrer cada semestre dentro de la región\n",
    "    for semestre, grupo_semestre in grupo_region.groupby('semestre'):\n",
    "        print(f\"Procesando duplicados para el semestre: {semestre} en la región {region}\")\n",
    "\n",
    "        # Asegurarse de que el grupo tiene al menos 2 filas para hacer comparaciones\n",
    "        if len(grupo_semestre) < 2:\n",
    "            continue\n",
    "\n",
    "        # Crear el vectorizador TF-IDF para las descripciones normalizadas del grupo por semestre\n",
    "        vectorizer = TfidfVectorizer()\n",
    "\n",
    "        # Transformar las descripciones normalizadas en una matriz TF-IDF\n",
    "        tfidf_matrix = vectorizer.fit_transform(grupo_semestre['descripcion_normalizada'])\n",
    "\n",
    "        # Calcular la similaridad coseno para las descripciones del grupo\n",
    "        similaridad_coseno = cosine_similarity(tfidf_matrix)\n",
    "\n",
    "        # Crear una columna 'duplicados_con' vacía para almacenar los IDs duplicados dentro del semestre\n",
    "        grupo_semestre['duplicados_con'] = [[] for _ in range(len(grupo_semestre))]\n",
    "\n",
    "        # Encontrar duplicados dentro del semestre\n",
    "        duplicados_region = []\n",
    "        for i in range(similaridad_coseno.shape[0]):\n",
    "            for j in range(similaridad_coseno.shape[1]):\n",
    "                # Evitar comparar una fila consigo misma y evitar duplicados repetidos\n",
    "                if i != j and similaridad_coseno[i, j] > umbral:\n",
    "                    id_1 = grupo_semestre.iloc[i]['id']\n",
    "                    id_2 = grupo_semestre.iloc[j]['id']\n",
    "\n",
    "                    # Verificar que el duplicado aún no está en la lista de duplicados\n",
    "                    if id_2 not in grupo_semestre.iloc[i]['duplicados_con']:\n",
    "                        duplicados_region.append((id_1, id_2))\n",
    "                        grupo_semestre.at[grupo_semestre.index[i], 'duplicados_con'].append(id_2)\n",
    "\n",
    "                    if id_1 not in grupo_semestre.iloc[j]['duplicados_con']:\n",
    "                        grupo_semestre.at[grupo_semestre.index[j], 'duplicados_con'].append(id_1)\n",
    "\n",
    "        # Guardar los duplicados encontrados para este semestre dentro de la región\n",
    "        duplicados_totales.extend(duplicados_region)\n",
    "\n",
    "        # Actualizar el DataFrame principal con los duplicados encontrados en el grupo por semestre\n",
    "        data_2021.update(grupo_semestre)\n",
    "\n",
    "# Eliminar duplicados en las listas de duplicados\n",
    "data_2021['duplicados_con'] = data_2021['duplicados_con'].apply(lambda x: list(set(x)))\n",
    "\n",
    "# Marcar los registros duplicados en la columna 'duplicado'\n",
    "data_2021['duplicado'] = data_2021['duplicados_con'].apply(lambda x: len(x) > 0)\n",
    "\n",
    "# Mostrar los pares de duplicados encontrados en todas las regiones y semestres\n",
    "print(f\"Pares de duplicados encontrados en todas las regiones y semestres: {len(duplicados_totales)}\")\n",
    "for dup in duplicados_totales:\n",
    "    print(f\"Duplicado entre id {dup[0]} y id {dup[1]}\")"
   ],
   "id": "e522f38ef76229d6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
